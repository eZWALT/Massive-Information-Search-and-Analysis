{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3 - User Relevance Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Document Relevance\n",
    "\n",
    "In this sesion we are going to implement a pseudo user relevance feedback on top of ElasticSearch\n",
    "\n",
    "One possibility that we have not used from the query results of ElasticSeach is the score computed as the relevance of the document respect to the terms of a query.\n",
    "\n",
    "You have the script `SearchIndexWeights.py` that allows searching for keywords in an index just like we do in any seach engine (like Google search or Bing).\n",
    "\n",
    "This script returns a limited number of hits and also shows the score of the documents (the documents are sorted by its score)\n",
    "\n",
    "**Read the first section** of the session documentation and play a little bit with the documents that you have in the `news` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "\n",
    "\n",
    "client = Elasticsearch()\n",
    "s = Search(using=client, index='news')\n",
    "\n",
    "\n",
    "q = Q('query_string',query='toronto')  # Feel free to change the word\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0:3].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print('ID= %s SCORE=%s' % (r.meta.id,  r.meta.score))\n",
    "    print('PATH= %s' % r.path)\n",
    "    print('TEXT: %s' % r.text[:50])\n",
    "    print('-----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2 Rocchio's Rule\n",
    "\n",
    "\n",
    "For implementing the relevance we are going to use the Rocchio's rule. We are going to extend the query for a number of interations using the terms in the more relevant documents that are retrieved.\n",
    "\n",
    "As is described in the session documentation you will need to write a scripts that given a query, repeats a number ($nrounds$) of times:\n",
    "\n",
    "1. Obtain the $k$ more relevant documents\n",
    "2. Compute a new query using the current query and the terms of the $k$ documents\n",
    "\n",
    "The Rocchio's rule involves computing the folowing:\n",
    "\n",
    "$$Query' = \t\\alpha \\times Query + \\beta \\times \\frac{d_1 + d_2 + \\cdots + d_k}{k}$$\n",
    "\n",
    "So we have different parameters to play with:\n",
    "\n",
    "1. The number of rounds ($nrounds$)\n",
    "2. The number of relevand documents ($k$)\n",
    "3. The parameters of the Rocchio's rule ($\\alpha$ and $\\beta$)\n",
    "4. The numbeer of terms in the recomputed query ($R$)\n",
    "\n",
    "**Read the documentation** and pay attention specially to how you have to build the query that you pass to ElasticSearch to include thw weights computed by the Rocchio's rule.\n",
    "\n",
    "Think that some of the elements that you need for this part are functions that you programmed already as part of the past session assignment.\n",
    "\n",
    "**Pay attention** to the documentation that you have to deliver for this session.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "\n",
    "import argparse\n",
    "\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "import numpy as np\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch.client import CatClient\n",
    "from elasticsearch_dsl.query import Q\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "import argparse\n",
    "__author__ = 'walter'\n",
    "\n",
    "def doc_count(client, index):\n",
    "    \"\"\"\n",
    "    Returns the number of documents in an index\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return int(CatClient(client).count(index=[index], format='json')[0]['count'])\n",
    "\n",
    "def document_term_vector(client, index, id):\n",
    "    \"\"\"\n",
    "    Returns the term vector of a document and its statistics a two sorted list of pairs (word, count)\n",
    "    The first one is the frequency of the term in the document, the second one is the number of documents\n",
    "    that contain the term\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :param id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    termvector = client.termvectors(index=index, id=id, fields=['text'],\n",
    "                                    positions=False, term_statistics=True)\n",
    "\n",
    "    file_td = {}\n",
    "    file_df = {}\n",
    "\n",
    "    if 'text' in termvector['term_vectors']:\n",
    "        for t in termvector['term_vectors']['text']['terms']:\n",
    "            file_td[t] = termvector['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            file_df[t] = termvector['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "    return sorted(file_td.items()), sorted(file_df.items())\n",
    "\n",
    "def toTFIDF(client, index, file_id):\n",
    "    \"\"\"\n",
    "    Returns the term weights of a document\n",
    "\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the frequency of the term in the document, and the number of documents\n",
    "    # that contain the term\n",
    "    file_tv, file_df = document_term_vector(client, index, file_id)\n",
    "\n",
    "    max_freq = max([f for _, f in file_tv])\n",
    "\n",
    "    dcount = doc_count(client, index)\n",
    "\n",
    "    tfidfw = {}\n",
    "    for (term, w),(_, df) in zip(file_tv, file_df):\n",
    "        #\n",
    "        idfi = np.log2((dcount/df))\n",
    "        tfdi = w/max_freq\n",
    "        tfidfw[term] = tfdi * idfi\n",
    "        # Something happens here\n",
    "        #\n",
    "\n",
    "    return normalize(tfidfw)\n",
    "\n",
    "def normalize(document):\n",
    "    summ = sum(document.values())\n",
    "    sqrt = np.sqrt(summ)\n",
    "    norm = {term: document.get(term, 0)/sqrt for term in set(document)}\n",
    "    return norm\n",
    "\n",
    "def search_file_by_path(client, index, path):\n",
    "    \"\"\"\n",
    "    Search for a file using its path\n",
    "\n",
    "    :param path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s = Search(using=client, index=index)\n",
    "    q = Q('match', path=path)  # exact search in the path field\n",
    "    s = s.query(q)\n",
    "    result = s.execute()\n",
    "\n",
    "    lfiles = [r for r in result]\n",
    "    if len(lfiles) == 0:\n",
    "        raise NameError(f'File [{path}] not found')\n",
    "    else:\n",
    "        return lfiles[0].meta.id\n",
    "\n",
    "#\n",
    "def get_dictionary_from_query(query):\n",
    "    dQuery = {}\n",
    "    for elem in query:\n",
    "\n",
    "        if '^' in elem:\n",
    "            key, val = elem.split('^')\n",
    "            val = float(val)\n",
    "\n",
    "        else:\n",
    "            val = 1.0\n",
    "            key = elem\n",
    "        \n",
    "        dQuery[key] = val\n",
    "        \n",
    "    return normalize(dQuery)\n",
    "\n",
    "def get_query_from_dictionary(theDict):\n",
    "    query = []\n",
    "\n",
    "    for elem in theDict:\n",
    "        q = elem + '^' + str(theDict[elem])\n",
    "        query.append(q)\n",
    "    \n",
    "    return query\n",
    "\n",
    "nrounds_study = {}\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--index', default=None, help='Index to search')\n",
    "    parser.add_argument('--k', default=10, type=int, help='Number of documents to return')\n",
    "    parser.add_argument('--beta', default=1, type=float, help=\"beta coefficient of Rocchio's rule\")\n",
    "    parser.add_argument('--alpha', default=2, type=float, help=\"Alpha coefficient of Rocchio's rule\")\n",
    "    parser.add_argument('--R', default=4, type=int, help=\"Number of R most important terms of a document to use in document fusion\")\n",
    "    parser.add_argument('--nrounds', default=100, type=int, help=\"Number of times Rocchio's law is applied to the original query\")\n",
    "    parser.add_argument('--query', default=None, nargs=argparse.REMAINDER, help='List of words to search')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    index = args.index\n",
    "    k = args.k\n",
    "    beta = args.beta\n",
    "    alpha = args.alpha \n",
    "    R = args.R \n",
    "    nrounds = args.nrounds \n",
    "    query = args.query\n",
    "    \n",
    "    try:\n",
    "        client = Elasticsearch()\n",
    "        s = Search(using=client, index=index)\n",
    "        \n",
    "        if query is not None:\n",
    "            for iteration in range(0, nrounds):\n",
    "                q = Q('query_string',query=query[0])\n",
    "                for i in range(1, len(query)):\n",
    "                    q &= Q('query_string',query=query[i])\n",
    "\n",
    "                print(query)\n",
    "                s = s.query(q)\n",
    "                response = s[0:k].execute()\n",
    "\n",
    "                dict_query = get_dictionary_from_query(query)\n",
    "                \n",
    "                x0 = 0\n",
    "                for term,val in dict_query.items():\n",
    "                    x0 = x0 + val \n",
    "                merged_documents = {}\n",
    "                \n",
    "                #Convert all the K most relevant documents to tfidf dictionaries and merge them into \n",
    "                for r in response:\n",
    "                    file_tw = toTFIDF(client, index, r.meta.id) # tf-idf\n",
    "                    merged_documents = {term: merged_documents.get(term, 0) + file_tw.get(term, 0) for term in set(merged_documents) | set(file_tw)} # sumem els valors de cada document\n",
    "                    print(f'ID= {r.meta.id} SCORE={r.meta.score}')\n",
    "                    print(f'PATH= {r.path}')\n",
    "                    print(f'TEXT: {r.text[:50]}')\n",
    "                    print(f'ITERATION: {iteration}')\n",
    "                    print('-----------------------------------------------------------------')\n",
    "                    \n",
    "                \n",
    "                #Apply Rocchio's rule \n",
    "                merged_documents = {term: merged_documents.get(term,0)*beta/k for term in set(merged_documents)} # B * merged_documents / k\n",
    "                old_query = {term: dict_query.get(term,0)*alpha for term in set(dict_query)} # a * query\n",
    "                new_query = {}\n",
    "                new_query = {term: merged_documents.get(term, 0) + old_query.get(term, 0) for term in set(merged_documents) | set(old_query)} # alpha * query + beta * merged_documents / K\n",
    "                \n",
    "                # sorterm and get the R most relevant terms, this can be done sorting or using priority queue in R*log(n) time\n",
    "                new_query = sorted(new_query.items(), key=operator.itemgetter(1), reverse = True) \n",
    "                x1 = 0\n",
    "                for (term,val) in new_query:\n",
    "                    x1 = x1 + val \n",
    "                    \n",
    "                nrounds_study[iteration] = abs(x1-x0)\n",
    "                new_query = new_query[:R] \n",
    "                # get query from dict\n",
    "                dict_query = dict((term, val) for (term, val) in new_query) \n",
    "                \n",
    "                \n",
    "                query = get_query_from_dictionary(normalize(dict_query))\n",
    "                print (f\"{response.hits.total['value']} Documents\")\n",
    "\n",
    "        else:\n",
    "            print('No query parameters passed')\n",
    "\n",
    "\n",
    "    except NotFoundError:\n",
    "        print(f'Index {index} does not exists')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nrounds plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'packaging'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "            \n",
    "        plt.plot(nrounds_study.keys(), nrounds_study.values())\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel('Y-axis label')\n",
    "        plt.title('Line Plot')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
